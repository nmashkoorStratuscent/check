{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook uses Google Cloud Storage instead of local storage**\n",
    "\n",
    "All data for current pipeline is sotred in root folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY = r'C:\\Users\\User\\Desktop\\Strat\\ml_pipeline-hafizur\\stratuscent\\combined\\keys\\testproject8008-24541b9591e3.json'\n",
    "BUCKET_NAME = 'data-store-strat'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break down of ML components for Training Pipeline implemented:\n",
    "\n",
    "1. Read Positive Data\n",
    "2. Read Negative Data\n",
    "3. Process Data\n",
    "    1. Merge Data\n",
    "    2. Pad Data\n",
    "4. Data_Split\n",
    "5. Model_Training\n",
    "6. Model Evaluation\n",
    "\n",
    "## Current Architecture Of ML-Pipeline\n",
    "\n",
    "![Architecture](./architecture.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Postive Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_postive_data(raw_files_path: str = None, analyte_name: str ='no2', run_name: str = 'no2', training:bool = True):\n",
    "    # Loading Dependencies\n",
    "    import os\n",
    "    import pathlib\n",
    "    import shutil\n",
    "    import pandas as pd\n",
    "    from tqdm import tqdm\n",
    "    from cloud_wrapper import GStore\n",
    "\n",
    "    \n",
    "    gstore = GStore(BUCKET_NAME)\n",
    "    # Checking for edge cases\n",
    "    if raw_files_path == None or analyte_name == None or run_name == None:\n",
    "        raise ValueError('Reading positive data failed: Component Configuration Invalid!')\n",
    "    if not gstore.is_dir(raw_files_path):\n",
    "        raise ValueError('Reading positive data failed: Raw files path do not exist')  \n",
    "    \n",
    "    # Read files\n",
    "    analyte_files = gstore.list_files(raw_files_path, recurse=True)\n",
    "    if len(analyte_files) == 0:\n",
    "        raise ValueError('Reading positive data failed: Raw files folder empty') \n",
    " \n",
    "    sensor_cols = [\"s\"+ str(i) for i in range(1, 33)]\n",
    "    removed_files = []\n",
    "\n",
    "    # Create Output folder\n",
    "    output_file_path = 'data/no2_positive'\n",
    "    if gstore.is_dir(output_file_path):\n",
    "         gstore.rmdir(output_file_path)\n",
    "    gstore.mkdir(output_file_path)\n",
    "\n",
    "    print(\"Num of {} files before filter: {}\".format(analyte_name, len(analyte_files)))\n",
    "    print('Processing files...')\n",
    "    if training:\n",
    "        for analyte_file in tqdm(analyte_files, colour='red'):\n",
    "            content = pd.read_csv(f'gs://{gstore.get_bucket_name()}/{analyte_file}')\n",
    "            if len(content[analyte_name].unique()) < 10:\n",
    "                removed_files.append(analyte_file)\n",
    "                continue\n",
    "            if not('run_name' in content.columns):\n",
    "                removed_files.append(analyte_file)\n",
    "                continue\n",
    "            if not('_'+ run_name + '_' in content.run_name[0]):\n",
    "                removed_files.append(analyte_file)\n",
    "                continue\n",
    "            for an_input in sensor_cols:\n",
    "                if an_input not in content.columns:\n",
    "                    removed_files.append(analyte_file)\n",
    "                    break \n",
    "        analyte_filtered_files = [x for x in analyte_files if x not in removed_files]\n",
    "        print(\"Num of {} files after filter: {}\".format(analyte_name, len(analyte_filtered_files)))\n",
    "\n",
    "        pos_files = analyte_filtered_files\n",
    "    else:\n",
    "        pos_files = analyte_files\n",
    "        print('Not training model so not filtering data')\n",
    "          \n",
    "    print('Moving positive data files...')\n",
    "    for x in tqdm(pos_files, colour='green',):\n",
    "        gstore.copy_file(x, f'{output_file_path}/{os.path.basename(x)}')\n",
    "    print('Moving Postive Data Completed!')\n",
    "read_postive_data('raw_data_sample/no2_positive', training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Function \n",
    "-   Traverses directory containing raw data files and filters only postive sample of a given Analyte of a particular run\n",
    "### Input\n",
    "-   Path to Folder containing raw data files\n",
    "-   Analyte name for which files are being filtered\n",
    "-   Run name under consideration\n",
    "-   Training flag: True if the files are going to \n",
    "    be used for training a new model or not\n",
    "    \n",
    "### Output\n",
    "-   Output folder path where read data is saved\n",
    "-   Output folder automatically created \n",
    "-   Saved Path: `data/no2_positive`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Negative Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Function \n",
    "-   Traverses directory containing raw data files and filters only negative sample of a given Analyte of a particular run\n",
    "### Input\n",
    "-   Path to Folder containing raw data files\n",
    "-   Analyte name for which files are being filtered\n",
    "-   Run name under consideration\n",
    "-   Training flag: True if the files are going to \n",
    "    be used for training a new model or not\n",
    "    \n",
    "### Output\n",
    "-   Output folder path where read data is saved\n",
    "-   Output folder automatically created \n",
    "-   Saved Path: `data/no2_negative`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_negative_data(raw_files_path: str = None, analyte_name: str ='no2', run_name: str = 'no2', training:bool = True):\n",
    "    # Loading Dependencies\n",
    "    import os\n",
    "    import pathlib\n",
    "    import shutil\n",
    "    import pandas as pd\n",
    "    from tqdm import tqdm\n",
    "    from cloud_wrapper import GStore\n",
    "\n",
    "    \n",
    "    gstore = GStore(BUCKET_NAME)\n",
    "    # Checking for edge cases\n",
    "    if raw_files_path == None or analyte_name == None or run_name == None:\n",
    "        raise ValueError('Reading negative data failed: Component Configuration Invalid!')\n",
    "    if not gstore.is_dir(raw_files_path):\n",
    "        raise ValueError('Reading negative data failed: Raw files path do not exist')  \n",
    "    \n",
    "    # Read files\n",
    "    analyte_files = gstore.list_files(raw_files_path, recurse=True)\n",
    "    if len(analyte_files) == 0:\n",
    "        raise ValueError('Reading negative data failed: Raw files folder empty') \n",
    " \n",
    "    sensor_cols = [\"s\"+ str(i) for i in range(1, 33)]\n",
    "    removed_files = []\n",
    "\n",
    "    # Create Output folder\n",
    "    output_file_path = 'data/no2_negative'\n",
    "    if gstore.is_dir(output_file_path):\n",
    "         gstore.rmdir(output_file_path)\n",
    "    gstore.mkdir(output_file_path)\n",
    "\n",
    "    print(\"Num of {} files before filter: {}\".format(analyte_name, len(analyte_files)))\n",
    "    print('Processing files...')\n",
    "    if training:\n",
    "        for neg_file in tqdm(analyte_files, colour='red'):\n",
    "            content =  pd.read_csv(f'gs://{gstore.get_bucket_name()}/{neg_file}')\n",
    "            if not('run_name' in content.columns):\n",
    "                removed_files.append(neg_file)\n",
    "                continue\n",
    "            if '_'+analyte_name+'_' in content.run_name[0]:\n",
    "                removed_files.append(neg_file)\n",
    "                continue\n",
    "            for an_input in sensor_cols:\n",
    "                if not(an_input in content.columns):\n",
    "                    removed_files.append(neg_file)\n",
    "                    break\n",
    "        analyte_filtered_files = [x for x in analyte_files if x not in removed_files]\n",
    "        print(\"Num of {} file after filter: {}\".format(analyte_name, len(analyte_filtered_files)))\n",
    "\n",
    "        neg_files = analyte_filtered_files\n",
    "    else:\n",
    "        neg_files = analyte_files\n",
    "        print('Not training model so not filtering data')\n",
    "          \n",
    "    print('Moving negative data files...')\n",
    "    for x in tqdm(neg_files, colour='green',):\n",
    "        gstore.copy_file(x, f'{output_file_path}/{os.path.basename(x)}')\n",
    "    print('Moving Negative Data Completed!')\n",
    "read_negative_data('raw_data_sample/no2_negative', training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function \n",
    "-   Traverses filtered positive and negative files to combines files: \n",
    "### Input\n",
    "-   Path to Folder containing filtered data files\n",
    "-   The path is to the parent directory. The components assumes that following two folders already exist:\n",
    "    - `no2_negative`\n",
    "    - `no2_positive`\n",
    "-   Analyte name under consideration\n",
    "-   Run name under consideration\n",
    "-   Training flag: True if the files are going to \n",
    "    be used for training a new model or not\n",
    "    \n",
    "### Output\n",
    "-   Output folder path where merge data is saved\n",
    "-   Output folder automatically created, deletes previously created folders\n",
    "-   Saved Path: `data/merged`\n",
    "-   The merged data is saved in the following way:\n",
    "    - Each data frame read is saved in `data/merged` folder as a csv file\n",
    "    - A accompanying json file is saved that stores the association between the each `trail_id` and associated `data file`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(data_folder: str = None, analyte_name: str ='no2', run_name: str = 'no2', training:bool = True):\n",
    "    # Loading Dependencies\n",
    "    import os\n",
    "    import pathlib\n",
    "    import shutil\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from tqdm import tqdm\n",
    "    from cloud_wrapper import GStore\n",
    "\n",
    "    gstore = GStore(BUCKET_NAME)\n",
    "    # Checking for edge cases\n",
    "    if data_folder == None or analyte_name == None or run_name == None:\n",
    "        raise ValueError('Merging data failed: Component Configuration Invalid!')\n",
    "\n",
    "    if not gstore.is_dir(f'{data_folder}/no2_positive') or not gstore.is_dir(f'{data_folder}/no2_negative'):\n",
    "        raise ValueError('Merging data failed: Data folder does not exist')\n",
    "    \n",
    "    # Read files\n",
    "    pos_files =  gstore.list_files(f'{data_folder}/no2_positive', recurse=True)\n",
    "    neg_files = gstore.list_files(f'{data_folder}/no2_negative', recurse=True)\n",
    "    all_files = pos_files + neg_files\n",
    "    \n",
    "    if len(all_files) == 0:\n",
    "        raise ValueError('Merge data failed: No files found')\n",
    "\n",
    "    all_data = {}\n",
    "\n",
    "    # Create output folders\n",
    "    output_folder_path ='data/merged'\n",
    "    if gstore.is_dir(output_folder_path):\n",
    "         gstore.rmdir(output_folder_path)\n",
    "    gstore.mkdir(output_folder_path)\n",
    "\n",
    "    print(\"Num of all files: {}\".format(len(all_files)))\n",
    "    print('Merging files...')\n",
    "    for idx, ctrial in enumerate(tqdm(all_files, colour='red')):\n",
    "        if 'checkpoint' in ctrial:\n",
    "            continue\n",
    "        df = pd.read_csv(f'gs://{gstore.get_bucket_name()}/{ctrial}')\n",
    "        df[\"filename\"] = os.path.basename(ctrial)[:-4]\n",
    "        if 'trial' in df.columns:\n",
    "            df.rename(columns={\"trial\": \"trial_id\"}, inplace = True)\n",
    "        try:\n",
    "            run = df.run_name.iloc[0]\n",
    "        except:\n",
    "            run = -1\n",
    "        if run in all_data:\n",
    "            all_data[run].append(f'{output_folder_path}/{str(df[\"filename\"].values[0])}.csv')\n",
    "        else:\n",
    "            all_data[run] = [f'{output_folder_path}/{str(df[\"filename\"].values[0])}.csv']\n",
    "        gstore.upload_from_memory(df.to_csv(index=False), f'{output_folder_path}/{str(df[\"filename\"].values[0])}.csv', content_type='text/csv')\n",
    "    \n",
    "    gstore.upload_from_memory(json.dumps(all_data), f'{output_folder_path}/all_data.json', content_type='application/json')\n",
    "    print('Merged files saved!')\n",
    "\n",
    "merge_data('data', training=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function \n",
    "- Reads the merged data and processes the data in two steps:\n",
    "    - Preparation stage\n",
    "    - Padding stage\n",
    "### Input\n",
    "-   Path to Folder containing filtered data files\n",
    "-   The path is to the parent directory. The components assumes that following sub folders already exist:\n",
    "    - `merged`\n",
    "-   Analyte name under consideration\n",
    "-   Run name under consideration\n",
    "-   Training flag: True if the files are going to \n",
    "    be used for training a new model or not\n",
    "-   Additional following paramters that determine how the data is processed\n",
    "    -   div_by=800\n",
    "    -   start_from=0\n",
    "    -   norm_bias_factor=-1 \n",
    "    -   sensormoduleid_or_deviceid='sensor_module_id', \n",
    "    -   zeros=True\n",
    "    -   max_len=None\n",
    "    \n",
    "### Output\n",
    "-   Output folder path where prepared data is saved\n",
    "-   Output folder automatically created, deletes previously created folders\n",
    "-   Saved Path: `data/processed`\n",
    "-   The processed data is saved in the following way:\n",
    "    - `meta_data.json` file: stores information about types and length of processed data headers\n",
    "    - `processed_data.json` file stores processed data that is stored as list\n",
    "    - `numpy/` directory is created and all headers having numpy arrays are saved in this directory as a seprate binary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(merged_data_path: str = None, analyte_name: str ='no2', run_name: str = 'no2', training=True,\n",
    "                        div_by=800, start_from=0, norm_bias_factor=-1, \n",
    "                        sensormoduleid_or_deviceid='sensor_module_id', \n",
    "                        zeros=True, max_len=None):\n",
    "    # Loading Dependencies\n",
    "    import os\n",
    "    import json\n",
    "    import pathlib\n",
    "    import shutil\n",
    "    import io\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from scipy.interpolate import interp1d\n",
    "    from scipy import interpolate\n",
    "    from tqdm import tqdm\n",
    "    from cloud_wrapper import GStore\n",
    "\n",
    "    # Helper Class\n",
    "    class NumpyEncoder(json.JSONEncoder):\n",
    "        \"\"\" Special json encoder for numpy types \"\"\"\n",
    "        def default(self, obj):\n",
    "            if isinstance(obj, np.integer):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, np.floating):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "    # Helper Function \n",
    "    def interpolate_vector(original_signal, desired_signal_length, kind=\"cubic\"):\n",
    "        \"\"\"Modifies the signal shape by sampling it based on the provided shape.\n",
    "\n",
    "        Args:\n",
    "            original_signal: np.ndarray, a numpy array containing the original signal. The original signal must be a 2d tensor.\n",
    "            desired_signal_length: int, specifying what is the length of the interpolated output signal.\n",
    "            kind: str, function to be used for fitting points on the signal. For more info refer to:\n",
    "                https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html \n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A numpy array containing the artificial signal, sampled from the original signal.\n",
    "        \"\"\"\n",
    "        if len(original_signal.shape) != 1:\n",
    "            raise ValueError(\"Function is specific to vectors! Input has {} dimensions.\".format(len(original_signal.shape)))\n",
    "        interp_func = interp1d(range(len(original_signal)), original_signal, kind=kind)\n",
    "        xnew = np.linspace(0, len(original_signal) - 1, desired_signal_length, endpoint=True)\n",
    "        interpolated = interp_func(xnew)\n",
    "        return interpolated\n",
    "\n",
    "    def interpolate_tensor2d(input_signals, desired_signal_length, axis=0, kind=\"cubic\"):\n",
    "        \"\"\"Modifies the signal shape by sampling it based on the provided shape.\n",
    "\n",
    "        Args:\n",
    "            input_signals: np.ndarray, a numpy array containing the original signal. The original signal must be a 2d tensor.\n",
    "            desired_signal_length: int, specifying what is the length of the interpolated output signal.\n",
    "            axis: int, axis which contains the sequence of the data.\n",
    "            kind: str, function to be used for fitting points on the signal. For more info refer to:\n",
    "                https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html \n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A numpy array containing the artificial signals, sampled from the original signals.\n",
    "        \"\"\"\n",
    "        if len(input_signals.shape) != 2:\n",
    "            raise ValueError(\"Function is specific to 2D tensors! Input has {} dimensions.\".format(len(input_signals.shape)))\n",
    "        original_signal = np.moveaxis(input_signals, axis, 1)\n",
    "        interpolated = []\n",
    "        for curr_sig in original_signal:\n",
    "            interpolated.append(interpolate_vector(curr_sig, desired_signal_length, kind))\n",
    "        interpolated = np.moveaxis(interpolated, 1, axis)\n",
    "        return interpolated\n",
    "\n",
    "    def get_length_adjusted_features(features, desired_len=None, print_progress_bar=False):\n",
    "        \"\"\"Modifies the length of sequence in features to have the same size. If desired_len is not selected, The median\n",
    "        value of all sequences lengths will be assigned as desired_length of final features. \n",
    "\n",
    "        Args:\n",
    "            features: np.ndarray, three dimensional numpy array containing the input signal.\n",
    "            desired_len: int, Specifying what should be the final length of sequences in features. If None is passed, the median\n",
    "                of all sequences is used. \n",
    "            print_progress_bar: bool, Flag to set progress bar on or off.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: numpy array containing features with the same sequence length.\n",
    "        \"\"\"\n",
    "        features = np.array(features)\n",
    "        if desired_len is None:\n",
    "            dimensions = np.array([len(cf) for cf in features])\n",
    "            desired_len = int(np.median(dimensions))\n",
    "        final_features = np.zeros((features.shape[0], desired_len, features[0].shape[-1]))\n",
    "        for i, cf in enumerate(features):\n",
    "            if print_progress_bar:\n",
    "                _print_progress_bar(i, len(features), \"Feature length adjustment:\")\n",
    "            current_features = cf\n",
    "            if len(cf) != desired_len:\n",
    "                current_features = interpolate_tensor2d(cf, desired_len, 0)\n",
    "            final_features[i] = current_features\n",
    "        return final_features\n",
    "    \n",
    "    gstore = GStore(BUCKET_NAME)\n",
    "    print('Loading merged data!')\n",
    "    sensor_cols =  sensor_cols = [\"s\"+ str(i) for i in range(1, 33)]\n",
    "     # Checking for edge cases\n",
    "    if merged_data_path == None or analyte_name == None or run_name == None:\n",
    "        raise ValueError('Data processing failed: Component Configuration Invalid!')\n",
    "\n",
    "    if not gstore.is_dir(f'{merged_data_path}/merged'):\n",
    "        raise ValueError('Data processing failed: Merged Data folder does not exist')\n",
    "    \n",
    "    if not gstore.is_file(f'{merged_data_path}/merged/all_data.json'):\n",
    "        raise ValueError('Data processing failed: Merged Data does not exist')\n",
    "    \n",
    "    # Read data\n",
    "    data_files_path = {}\n",
    "    all_data = {}\n",
    "    data_files_path = json.loads(gstore.download_into_memory(f'{merged_data_path}/merged/all_data.json'))\n",
    "    \n",
    "    if sum(map(lambda x : len(x) if type(x) == list else 0, data_files_path.values())) == 0:\n",
    "        raise ValueError('Data processing failed: Merged Data file format error')\n",
    "\n",
    "    for run_id in tqdm(data_files_path, colour='red'):\n",
    "       all_data[run_id] = list(map(lambda x: pd.read_csv(f'gs://{gstore.get_bucket_name()}/{x}'), data_files_path[run_id]))\n",
    "\n",
    "    # Create output folders\n",
    "    output_folder_path ='data/processed'\n",
    "    if gstore.is_dir(output_folder_path):\n",
    "         gstore.rmdir(output_folder_path)\n",
    "    gstore.mkdir(output_folder_path)\n",
    "\n",
    "    # Create numpy output folders\n",
    "    numpy_folders_output = f'{output_folder_path}/numpy'\n",
    "    if gstore.is_dir(numpy_folders_output):\n",
    "         gstore.rmdir(numpy_folders_output)\n",
    "    gstore.mkdir(numpy_folders_output)\n",
    "\n",
    "    # Create temp folder\n",
    "    if pathlib.Path('tmp').is_dir():\n",
    "       shutil.rmtree('tmp')\n",
    "    pathlib.Path('tmp').mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "    print('     Initiating data preparation stage!')\n",
    "    ds = {\"run\":[], \"sensor_responses\":[], \"labels\": [], \"extra_inputs\": [], 'last_index': [],\n",
    "                \"moduleID\": [], \"humidity\": [], \"temperature\": [], \"trial_id\": [], 'original_label_values': [], \"filename\": []}\n",
    "    for k in all_data:\n",
    "        for trial_id, cdata in enumerate(tqdm(all_data[k], colour='red')):\n",
    "\n",
    "            fin_labels = np.zeros((len(cdata[::]), 1))\n",
    "            if run_name not in cdata.run_name[0].lower():\n",
    "                all_labels = np.zeros(len(cdata.iloc[:,0]))\n",
    "            else:\n",
    "                cdata[analyte_name] = cdata[analyte_name] - min(cdata[analyte_name])\n",
    "                cdata[analyte_name] = cdata[analyte_name].div(div_by)\n",
    "                all_labels = cdata[analyte_name].values\n",
    "\n",
    "            fin_labels[:, 0]  = all_labels[::] \n",
    "\n",
    "            baseline = np.median(cdata[sensor_cols].iloc[start_from:start_from+100],axis=0).reshape(1, len(sensor_cols))\n",
    "\n",
    "            if norm_bias_factor is None:\n",
    "                norm = cdata[sensor_cols].values\n",
    "            else:\n",
    "                norm = cdata[sensor_cols].values/baseline + norm_bias_factor\n",
    "            norm = norm[::]\n",
    "\n",
    "            ###############################################\n",
    "            if training:\n",
    "                random_adjust = np.random.randint(100, 500)\n",
    "                section_to_adjust = norm[0:200].reshape(1,200,len(sensor_cols)) #takes the first 200 rows of the normalized data\n",
    "                section_to_adjust = get_length_adjusted_features(section_to_adjust, random_adjust)[0] # interpolate and extends them to the desired length\n",
    "                norm = np.concatenate((section_to_adjust, norm[200:]), axis=0) # replace the first potion with the extended data\n",
    "                labels_to_adjust = np.zeros((random_adjust, 1))\n",
    "                fin_labels = np.concatenate((labels_to_adjust, fin_labels[200:]), axis=0)\n",
    "                adjusted = {}\n",
    "                for column in [\"humidity\", \"temperature\"]:\n",
    "                    section_to_adjust = cdata[column][::][start_from:][:200].values.reshape(1,200,1)\n",
    "                    section_to_adjust = get_length_adjusted_features(section_to_adjust, random_adjust)[0]\n",
    "                    adjust_len = section_to_adjust.shape[0]\n",
    "                    length = cdata[column][::][start_from:][200:].values.shape[0]\n",
    "                    adjusted[column] = np.concatenate((section_to_adjust, cdata[column][::][start_from:][200:].values.reshape(length,1)), axis=0).reshape(adjust_len + length)\n",
    "            ##############################################\n",
    "                ds[\"sensor_responses\"].append(norm[start_from:, :])\n",
    "                ds['last_index'].append(len(cdata)+random_adjust-200)\n",
    "                ds[\"labels\"].append(fin_labels[start_from:])\n",
    "                ds[\"humidity\"].append(adjusted[\"humidity\"])\n",
    "                ds[\"temperature\"].append(adjusted[\"temperature\"])\n",
    "            else:\n",
    "                ds[\"sensor_responses\"].append(norm[start_from:, :])\n",
    "                ds['last_index'].append(len(cdata))\n",
    "                ds[\"labels\"].append(fin_labels[start_from:])\n",
    "                ds[\"humidity\"].append(cdata[\"humidity\"].to_numpy())\n",
    "                ds[\"temperature\"].append(cdata[\"temperature\"].to_numpy())\n",
    "            try:\n",
    "                ds[\"moduleID\"].append(cdata[sensormoduleid_or_deviceid].iloc[0])\n",
    "            except:\n",
    "                ds[\"moduleID\"].append(cdata[\"sensor_module_id\"].iloc[0])\n",
    "            try:\n",
    "                ds[\"trial_id\"].append(cdata[\"trial_id\"].iloc[0])\n",
    "            except:\n",
    "                ds[\"trial_id\"].append(-1)\n",
    "            ds[\"run\"].append(k)\n",
    "            ds[\"filename\"].append(cdata[\"filename\"].iloc[0])\n",
    "\n",
    "    print('     Initiating data padding stage!')\n",
    "\n",
    "    if max_len is None:\n",
    "            max_len = max([len(trial) for trial in ds[\"sensor_responses\"]])\n",
    "    try:\n",
    "        if zeros:\n",
    "            new_data = np.negative(np.ones((len(ds[\"sensor_responses\"]), max_len, len(sensor_cols))))\n",
    "            new_labels = np.zeros((len(ds[\"labels\"]), max_len, ds[\"labels\"][0].shape[1]))\n",
    "            new_humidity = np.zeros((len(ds[\"humidity\"]), max_len))\n",
    "            new_temperature = np.zeros((len(ds[\"temperature\"]), max_len))\n",
    "\n",
    "        else:\n",
    "            new_data = np.negative(np.ones((len(ds[\"sensor_responses\"]), max_len, len(sensor_cols))))\n",
    "            new_labels = np.ones((len(ds[\"labels\"]), max_len, ds[\"labels\"][0].shape[1]))\n",
    "            new_humidity = np.ones((len(ds[\"humidity\"]), max_len))\n",
    "            new_temperature = np.ones((len(ds[\"temperature\"]), max_len))\n",
    "\n",
    "        for i, trial in enumerate(tqdm(ds[\"sensor_responses\"], colour='red')):\n",
    "                if len(trial) > max_len:\n",
    "                    length = max_len\n",
    "                else:\n",
    "                    length = len(trial)\n",
    "                new_data[i, :length, :] = trial[:length, :]\n",
    "                new_labels[i, :length, :] = ds[\"labels\"][i][:length]\n",
    "                new_humidity[i, :length] = ds[\"humidity\"][i][:length]\n",
    "                new_temperature[i, :length] = ds[\"temperature\"][i][:length]\n",
    "    except:\n",
    "        raise ValueError('Data processing failed: Data padding failed')\n",
    "\n",
    "    ds[\"sensor_responses\"] = new_data\n",
    "    ds[\"labels\"] = new_labels\n",
    "    ds[\"humidity\"] = new_humidity\n",
    "    ds[\"temperature\"] = new_temperature\n",
    "\n",
    "    print('     Saving Data!')\n",
    "    # Saving meta_data\n",
    "    meta_data = {}\n",
    "    processed_data = {}\n",
    "    for i in tqdm(ds, colour='green'):\n",
    "        if len(ds[i]) > 0 and type(ds[i][0]) == np.ndarray:\n",
    "            meta_data[i] = True, len(ds[i]), f'{numpy_folders_output}/{i}.npy'\n",
    "            np.save(os.path.join('tmp',f'{i}.npy'), ds[i], allow_pickle=True)\n",
    "            gstore.upload_file_local(os.path.join('tmp',f'{i}.npy'), f'{numpy_folders_output}/{i}.npy')\n",
    "        else:\n",
    "            meta_data[i] = False, len(ds[i])\n",
    "            processed_data[i] = ds[i]\n",
    "\n",
    "    gstore.upload_from_memory(json.dumps(meta_data), f'{output_folder_path}/meta_data.json', content_type='application/json')\n",
    "    gstore.upload_from_memory(json.dumps(processed_data, cls=NumpyEncoder), f'{output_folder_path}/processed_data.json', content_type='application/json')\n",
    "    \n",
    "    # Destroy local temp directory\n",
    "    if pathlib.Path('tmp').is_dir():\n",
    "       shutil.rmtree('tmp')\n",
    "    print('Data processing complete!')\n",
    "process_data('data', training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function \n",
    "- Splits the processed data into train and test idx\n",
    "\n",
    "### Input\n",
    "-   Path to Folder containing processed data \n",
    "-   The path is to the parent directory. The components assumes that following sub folders already exist:\n",
    "    - `processed`\n",
    "-   Training flag: True if the files are going to \n",
    "    be used for training a new model or not\n",
    "    \n",
    "### Output\n",
    "-   Output folder path where split_data idx are saved\n",
    "-   Output folder automatically created, deletes previously created folders\n",
    "-   Saved Path: `data/split_data_idx`\n",
    "-   The processed data is saved in the following way:\n",
    "    - `train_idx` numpy file containing indices used for training\n",
    "    - `test_idx`  numpy file containing indices used for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(processed_data_path: str = None, training:bool = True):\n",
    "    # Loading Dependencies\n",
    "    import os\n",
    "    import json\n",
    "    import pathlib\n",
    "    import shutil\n",
    "    import io\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from tqdm import tqdm\n",
    "    from cloud_wrapper import GStore\n",
    "\n",
    "    # Helper Functions\n",
    "    def remove_dupl(ds):\n",
    "        print('Remove duplicates')\n",
    "        for key in ds.keys():\n",
    "            print(key, \":   \", type(ds[key]))\n",
    "\n",
    "        values_to_remove = []\n",
    "\n",
    "        for i, row in enumerate(ds[\"labels\"]):\n",
    "            if np.isnan(row).any():# or np.sum(ds[\"labels\"][i] < 50) == 0:\n",
    "                values_to_remove.append(i)    \n",
    "\n",
    "        final_indices = np.array([x for x in range(len(ds[\"labels\"])) if x not in values_to_remove])\n",
    "\n",
    "        for val in values_to_remove:\n",
    "            del ds[\"run\"][val]\n",
    "            # del ds[\"extra_inputs\"][val]\n",
    "            del ds[\"moduleID\"][val]\n",
    "            del ds[\"trial_id\"][val]\n",
    "        for key in [\"labels\", \"humidity\", \"temperature\", \"sensor_responses\"]:\n",
    "            ds[key] = ds[key][final_indices]\n",
    "    \n",
    "    gstore = GStore(BUCKET_NAME)\n",
    "    print('Splitting data...')\n",
    "    # Checking for edge cases\n",
    "    if processed_data_path == None:\n",
    "        raise ValueError('Splitting data failed: Component Configuration Invalid!')\n",
    "\n",
    "    if not gstore.is_dir(f'{processed_data_path}/processed'):\n",
    "        raise ValueError('Merging data failed: Processed does not exist')\n",
    "\n",
    "    if not gstore.is_dir(f'{processed_data_path}/processed/numpy'):\n",
    "        raise ValueError('Merging data failed: Processed data corrupted')\n",
    "\n",
    "    if not gstore.is_file(f'{processed_data_path}/processed/meta_data.json'):\n",
    "        raise ValueError('Merging data failed: Processed data meta-data missing')    \n",
    "\n",
    "    if not gstore.is_file(f'{processed_data_path}/processed/processed_data.json'):\n",
    "        raise ValueError('Merging data failed: Processed data missing')  \n",
    "\n",
    "    # Read processed data\n",
    "    meta_data = json.loads(gstore.download_into_memory(f'{processed_data_path}/processed/meta_data.json'))\n",
    "    ds = json.loads(gstore.download_into_memory(f'{processed_data_path}/processed/processed_data.json'))   \n",
    "\n",
    "    for header in tqdm(meta_data, colour='red'):\n",
    "        if meta_data[header][0]:\n",
    "            ds[header] = np.load(io.BytesIO(gstore.download_into_memory(meta_data[header][2])), allow_pickle=True)\n",
    "        \n",
    "    # Create output folders\n",
    "    output_folder_path = 'data/split_data_idx'\n",
    "    if gstore.is_dir(output_folder_path):\n",
    "         gstore.rmdir(output_folder_path)\n",
    "    gstore.mkdir(output_folder_path)\n",
    "\n",
    "    if training == True:\n",
    "        remove_dupl(ds)\n",
    "        test_perc = 0.2\n",
    "    else:\n",
    "        test_perc = 1\n",
    "\n",
    "    all_idx = np.arange(len(ds[\"run\"]))\n",
    "    test_size = int(len(ds[\"run\"]) * test_perc)\n",
    "    \n",
    "    np.random.shuffle(all_idx)\n",
    "    \n",
    "    test_idx = all_idx[:test_size]\n",
    "    train_idx = all_idx[test_size:]\n",
    "    \n",
    "    temp_bytes = io.BytesIO()\n",
    "    np.save(temp_bytes, train_idx, allow_pickle=True)\n",
    "    gstore.upload_from_memory(temp_bytes.getvalue(), f'{output_folder_path}/train_idx.npy', content_type='application/octet-stream')\n",
    "    \n",
    "    temp_bytes = io.BytesIO()\n",
    "    np.save(temp_bytes, test_idx, allow_pickle=True)\n",
    "    gstore.upload_from_memory(temp_bytes.getvalue(), f'{output_folder_path}/test_idx.npy', content_type='application/octet-stream')\n",
    "        \n",
    "    print('Data split!')\n",
    "split_data('data', training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function \n",
    "- Uses processed data and data split information to train new model\n",
    "\n",
    "### Input\n",
    "-   Path to Folder containing processed data \n",
    "-   The path is to the parent directory. The components assumes that following sub folders already exist:\n",
    "    - `processed`\n",
    "-   Path to Folder containing data split files \n",
    "-   The path is to the parent directory. The components assumes that following sub folders already exist:\n",
    "    - `split_data_idx` \n",
    "-   Additional following parameters to determine the training:\n",
    "    -   analyte_name, \n",
    "    -   epochs\n",
    "    -   batch_size\n",
    "    -   div_by    \n",
    "### Output\n",
    "-   Output folder path where model is saved\n",
    "-   Output folder automatically created, deletes previously created folders\n",
    "-   Saved Path: `models/models_path)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(processed_data_path: str = None, data_split_path: str = None, analyte_name: str = 'no2', \n",
    "                epochs: int = 1, max_len:int = 10000, div_by:int = 800, before_after:int = 100, \n",
    "                start_from: int = 0, norm_bias_factor: int = -1, batch_size:int = 16, layers:int = 32,\n",
    "                learning_rate:float = 5e-3\n",
    "                ):\n",
    "    # Loading Dependencies\n",
    "    import os\n",
    "    import pathlib\n",
    "    import shutil\n",
    "    import time\n",
    "    import json\n",
    "    import io\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    import tensorflow.keras.backend as K\n",
    "    from tqdm import tqdm\n",
    "    from cloud_wrapper import GStore\n",
    "\n",
    "    # Helper Functions\n",
    "    def get_callbacks(model_name):\n",
    "        callbacks = []\n",
    "        callbacks.append(tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=10, verbose=0, mode=\"auto\", baseline=None))\n",
    "        callbacks.append(tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', factor=0.1, patience=4, verbose=1, mode='auto', min_delta=0.000001,\n",
    "            cooldown=0, min_lr=.0000001))\n",
    "        callbacks.append(tf.keras.callbacks.ModelCheckpoint(\n",
    "            model_name, monitor=\"val_loss\", verbose=1, save_best_only=True, mode=\"auto\",\n",
    "            save_weights_only=True))\n",
    "        return callbacks\n",
    "\n",
    "    def get_model_sequential(batch_input_shape, layers=20, number_of_classes=1, stateful=False):\n",
    "        dropouts = 0.2\n",
    "\n",
    "        inlayer = tf.keras.Input(shape=batch_input_shape[1:], batch_size=batch_input_shape[0], name=\"input1\")\n",
    "        shared1 = tf.compat.v1.keras.layers.LSTM(layers, return_sequences=True, stateful=stateful, #go_backwards=True,\n",
    "                                                    batch_input_shape=batch_input_shape)(inlayer)\n",
    "        shared1 = tf.keras.layers.BatchNormalization()(shared1)\n",
    "        shared1 = tf.keras.layers.Dropout(dropouts)(shared1)\n",
    "\n",
    "        shared1 = tf.compat.v1.keras.layers.LSTM(layers, return_sequences=True, stateful=stateful, #go_backwards=True,\n",
    "                                                    batch_input_shape=batch_input_shape)(shared1)\n",
    "        shared1 = tf.keras.layers.BatchNormalization()(shared1)\n",
    "        shared1 = tf.keras.layers.Dropout(dropouts)(shared1)\n",
    "\n",
    "        shared2 = tf.keras.layers.Dense(layers, activation='relu')(inlayer)\n",
    "        shared2 = tf.keras.layers.BatchNormalization()(shared2)\n",
    "        shared2 = tf.keras.layers.Dropout(dropouts)(shared2)\n",
    "\n",
    "        combined = tf.keras.layers.Add()([shared1, shared2])\n",
    "\n",
    "        x = tf.keras.layers.Dense(layers-4, activation='relu')(combined)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(dropouts)(x)\n",
    "        x_out = tf.keras.layers.Dense(1, activation='relu')(x)\n",
    "        model = tf.keras.models.Model(inputs=[inlayer], outputs=[x_out])\n",
    "        return model\n",
    "\n",
    "    def train_model(model, ds, analyte_name, train_idx, epochs, batch_size=2):\n",
    "        # Specify which run to select (it's an index not the actual run number).\n",
    "        run_index = 1\n",
    "        data = ds[\"sensor_responses\"]\n",
    "        print(data.shape)\n",
    "        labels_analyte = ds[\"labels\"]\n",
    "        labels_humidity = ds[\"humidity\"]\n",
    "        labels_temperature = ds[\"temperature\"]\n",
    "\n",
    "        train_data = data[train_idx]\n",
    "        train_labels_analyte = labels_analyte[train_idx]\n",
    "        train_labels_humidity = labels_humidity[train_idx]\n",
    "        train_labels_temperature = labels_temperature[train_idx]\n",
    "\n",
    "        model_name = \"{}_model.h5\".format(analyte_name)\n",
    "        train_frac = int(len(train_data)*.8)\n",
    "\n",
    "        if train_frac %2 != 0:\n",
    "            train_frac += 1\n",
    "        val_frac = train_frac\n",
    "        if (len(train_data) - val_frac) % 2 != 0:\n",
    "            val_frac += 2\n",
    "        callback = get_callbacks(model_name)\n",
    "\n",
    "        new_train_data = np.zeros((train_data.shape[0], train_data.shape[1], ds[\"sensor_responses\"][0].shape[1]))\n",
    "        new_train_data[:, :, :ds[\"sensor_responses\"][0].shape[1]] = train_data\n",
    "\n",
    "        train_data = new_train_data\n",
    "        print(\"Size of training data: \", train_frac)\n",
    "        print(\"Size of validation data: \", len(train_data[val_frac+1+2:]))\n",
    "        print(\"Size of validation data: \", len(train_labels_analyte[val_frac+1+2:]))\n",
    "        model.fit(train_data[:train_frac],\n",
    "                            [\n",
    "                                train_labels_analyte[:train_frac],\n",
    "\n",
    "                                ],\n",
    "                            batch_size=batch_size, epochs=epochs, callbacks=callback, shuffle=True,\n",
    "                            validation_data=([\n",
    "                                train_data[val_frac+1+2:],\n",
    "                            ],\n",
    "                            [\n",
    "                                train_labels_analyte[val_frac+1+2:],\n",
    "                            ]))\n",
    "        return model\n",
    "\n",
    "    def custom_loss(y_true, y_pred):\n",
    "        return K.mean(K.square(K.abs(y_true - y_pred)))\n",
    "    \n",
    "    gstore = GStore(BUCKET_NAME)\n",
    "    print('Starting Model Training...')\n",
    "    # Checking for edge cases\n",
    "    if processed_data_path == None or data_split_path == None:\n",
    "        raise ValueError('Training failed: Component Configuration Invalid!')\n",
    "\n",
    "    if not gstore.is_dir(f'{processed_data_path}/processed'):\n",
    "        raise ValueError('Training failed: Processed does not exist')\n",
    "\n",
    "    if not gstore.is_dir(f'{processed_data_path}/processed/numpy'):\n",
    "        raise ValueError('Training failed: Processed data corrupted')\n",
    "\n",
    "    if not gstore.is_file(f'{processed_data_path}/processed/meta_data.json'):\n",
    "        raise ValueError('Training failed: Processed data meta-data missing')    \n",
    "\n",
    "    if not gstore.is_file(f'{processed_data_path}/processed/processed_data.json'):\n",
    "        raise ValueError('Training failed: Processed data missing')  \n",
    "\n",
    "    # Read processed data\n",
    "    meta_data = json.loads(gstore.download_into_memory(f'{processed_data_path}/processed/meta_data.json'))\n",
    "    ds = json.loads(gstore.download_into_memory(f'{processed_data_path}/processed/processed_data.json'))   \n",
    "\n",
    "    for header in meta_data:\n",
    "        if meta_data[header][0]:\n",
    "            ds[header] = np.load(io.BytesIO(gstore.download_into_memory(meta_data[header][2])), allow_pickle=True)\n",
    "\n",
    "    if not gstore.is_file(f'{data_split_path}/split_data_idx/train_idx.npy'):\n",
    "        raise ValueError('Training failed: Data splits could not be load')  \n",
    "        \n",
    "    if not gstore.is_file(f'{data_split_path}/split_data_idx/test_idx.npy'):\n",
    "        raise ValueError('Training failed: Data splits could not be load')  \n",
    "    \n",
    "    test_idx = np.load(io.BytesIO(gstore.download_into_memory(f'{data_split_path}/split_data_idx/test_idx.npy')), allow_pickle=True)\n",
    "    train_idx = np.load(io.BytesIO(gstore.download_into_memory(f'{data_split_path}/split_data_idx/train_idx.npy')), allow_pickle=True)\n",
    "\n",
    "    # Create output folders\n",
    "    cur_time =  time.strftime(\"_%Y_%m_%d_%H_%M_%S\")\n",
    "    local_model_path = os.path.join('tmp', 'models', cur_time)\n",
    "    model_path = f'models/{cur_time}'\n",
    "    latest_model_path = f'models/latest-{cur_time}'\n",
    "    # Check on cloud\n",
    "    if gstore.is_dir(model_path):\n",
    "         gstore.rmdir(model_path)\n",
    "    gstore.mkdir(model_path)\n",
    "    # Create temp local dorectory\n",
    "    if pathlib.Path(local_model_path).is_dir():\n",
    "       shutil.rmtree(local_model_path)\n",
    "    pathlib.Path(local_model_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Train Model\n",
    "    model = get_model_sequential((None, None, ds[\"sensor_responses\"][0].shape[1]), layers=layers)\n",
    "    model.compile(optimizer=tf.optimizers.Adam(learning_rate), loss=custom_loss, metrics=[tf.keras.losses.MeanSquaredError()])\n",
    "    model = train_model(model, ds, analyte_name=analyte_name, train_idx=train_idx, epochs=epochs, batch_size=batch_size)\n",
    "    model_name = \"{}_model.h5\".format(analyte_name)\n",
    "    model_json = model.to_json()\n",
    "    with open(os.path.join(local_model_path, model_name), \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(os.path.join(local_model_path, model_name))\n",
    "    model.save(os.path.join(local_model_path, model_name))\n",
    "\n",
    "    # Upload to cloud\n",
    "    gstore.upload_file_local(f'{local_model_path}/{model_name}', f'{model_path}/{model_name}')\n",
    "    gstore.upload_from_memory(cur_time.encode('utf-8'), f'models/latest.txt', content_type='text/plain')\n",
    "    # Destroy local temp directory\n",
    "    if pathlib.Path('tmp').is_dir():\n",
    "       shutil.rmtree('tmp')\n",
    "       \n",
    "    print('Training Complete!')\n",
    "model_training('data', 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function \n",
    "- Use processed data to evaluate performance of a trained model\n",
    "\n",
    "### Input\n",
    "-   Path to Folder containing processed data\n",
    "-   The path is to the parent directory. The components assumes that following sub folders already exist:\n",
    "    - `processed`\n",
    "-   Path to Folder containing split data files \n",
    "-   The path is to the parent directory. The components assumes that following sub folders already exist:\n",
    "    - `split_data_idx`\n",
    "-   Path to the folder containing model file\n",
    "    - The folder itself will be named based on the time the model was trained on and the weights will have the followuing name:\n",
    "    `no2_models.h5`\n",
    "-   Ground Truth flag: True if the the data used for evaluation has ground truth labels\n",
    "-   Addtional parameter required for creating predicted labels\n",
    "    - div_by = 800\n",
    "\n",
    "### Output\n",
    "-   Output folder path where evaluation results are saved\n",
    "-   Output folder automatically created, deletes previously created folders\n",
    "-   Saved Path: `results/`\n",
    "    -   `csv/`\n",
    "    -   `plots/`\n",
    "    -   `results.path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(processed_data_path: str = None, data_split_path: str = None, model_path:str=None, groundtruth:bool=True,\n",
    "            div_by:int=800, layers:int=32):\n",
    "   # Loading Dependencies\n",
    "    import json\n",
    "    import os\n",
    "    import shutil\n",
    "    import pathlib\n",
    "    import re\n",
    "    import time\n",
    "    import csv\n",
    "    import io\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import r2_score, mean_squared_error\n",
    "    from cloud_wrapper import GStore\n",
    "    from uuid import uuid4\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    # Variables\n",
    "    analytes =  [\"no2\", \"no\", \"nh3\", \"dry_vape\", \"ethanol\"]\n",
    "    # Helper Functions\n",
    "    def plot_pred(pp, ds, idx, plot_path, csv_path, groundtruth=True):\n",
    "        fig, axes = plt.subplots(2,2, figsize=(15, 10), dpi=50, constrained_layout=True)\n",
    "        last_idx = ds['last_index'][idx]\n",
    "\n",
    "        for index_row in range(2):\n",
    "            for index_col in range(2):\n",
    "                if index_row == 0 and index_col == 0:\n",
    "                    axes[index_row][index_col].set_title('prediction')\n",
    "                    axes[index_row][index_col].plot(ds[\"labels\"][idx][2:last_idx], label=\"actual_analyte\")\n",
    "                    axes[index_row][index_col].plot(pp[2:last_idx], label=\"pred_analyte\")\n",
    "\n",
    "                    axes[index_row][index_col].legend()\n",
    "                elif index_row == 0 and index_col == 1:\n",
    "                    axes[index_row][index_col].set_title('temperature')\n",
    "                    axes[index_row][index_col].plot(ds['temperature'][idx][2:last_idx], 'blue', label='baseline')\n",
    "                    axes[index_row][index_col].legend()\n",
    "                elif index_row == 1 and index_col == 0:\n",
    "                    axes[index_row][index_col].set_title('sensor_responses')\n",
    "                    axes[index_row][index_col].plot(ds['sensor_responses'][idx][2:last_idx])\n",
    "                else:       \n",
    "                    axes[index_row][index_col].set_title('humidity')\n",
    "                    axes[index_row][index_col].plot(ds['humidity'][idx][2:last_idx], 'blue', label='baseline')\n",
    "                    axes[index_row][index_col].legend()\n",
    "        new_line = '\\n'\n",
    "        module_id = ds['moduleID'][idx]\n",
    "        run_id = ds['run'][idx]\n",
    "        trial = ds['trial_id'][idx]\n",
    "        filename = ds['filename'][idx]\n",
    "        fig.suptitle(f'{module_id} \\n'\n",
    "                     f'{run_id} \\n'\n",
    "                     f'{trial} \\n'\n",
    "                     f'{filename}')\n",
    "        file_name = r\"plot_{}_{}_{}.png\".format(ds['moduleID'][idx], ds['run'][idx], ds['trial_id'][idx])\n",
    "        if gstore.is_file(f'{plot_path}/{file_name}'):\n",
    "            file_name = os.path.splitext(file_name)[0] + str(uuid4()) + '.png'\n",
    "        # Save figure image to a bytes buffer\n",
    "        buf = io.BytesIO()\n",
    "        fig.savefig(buf, bbox_inches='tight', facecolor='white')\n",
    "        gstore.upload_from_memory(buf.read(), f'{plot_path}/{file_name}', content_type='image/png')\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Write results in .csv file\n",
    "        results_to_csv = pd.DataFrame(ds['labels'][idx][0:last_idx])\n",
    "        results_to_csv.columns = ['ground_truth']\n",
    "        results_to_csv['prediction'] = pp[0:last_idx]\n",
    "        results_to_csv['filename'] = ds['filename'][idx]\n",
    "        results_to_csv['module_ID'] = ds['moduleID'][idx]\n",
    "        results_to_csv['run_ID'] = ds['run'][idx]\n",
    "        results_to_csv['trial_ID'] = ds['trial_id'][idx]\n",
    "        results_to_csv.to_csv(f'gs://{gstore.get_bucket_name()}/{csv_path}/{ds[\"filename\"][idx]}_predictions.csv', header=['ground_truth', 'prediction', 'filename', 'module_ID', 'run_ID', 'trial_ID'], index = False)\n",
    "   \n",
    "    def get_model_sequential(batch_input_shape, layers=20, number_of_classes=1, stateful=False):\n",
    "        dropouts = 0.2\n",
    "\n",
    "        inlayer = tf.keras.Input(shape=batch_input_shape[1:], batch_size=batch_input_shape[0], name=\"input1\")\n",
    "        shared1 = tf.compat.v1.keras.layers.LSTM(layers, return_sequences=True, stateful=stateful, #go_backwards=True,\n",
    "                                                    batch_input_shape=batch_input_shape)(inlayer)\n",
    "        shared1 = tf.keras.layers.BatchNormalization()(shared1)\n",
    "        shared1 = tf.keras.layers.Dropout(dropouts)(shared1)\n",
    "\n",
    "        shared1 = tf.compat.v1.keras.layers.LSTM(layers, return_sequences=True, stateful=stateful, #go_backwards=True,\n",
    "                                                    batch_input_shape=batch_input_shape)(shared1)\n",
    "        shared1 = tf.keras.layers.BatchNormalization()(shared1)\n",
    "        shared1 = tf.keras.layers.Dropout(dropouts)(shared1)\n",
    "\n",
    "        shared2 = tf.keras.layers.Dense(layers, activation='relu')(inlayer)\n",
    "        shared2 = tf.keras.layers.BatchNormalization()(shared2)\n",
    "        shared2 = tf.keras.layers.Dropout(dropouts)(shared2)\n",
    "\n",
    "        combined = tf.keras.layers.Add()([shared1, shared2])\n",
    "\n",
    "        x = tf.keras.layers.Dense(layers-4, activation='relu')(combined)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(dropouts)(x)\n",
    "        x_out = tf.keras.layers.Dense(1, activation='relu')(x)\n",
    "        model = tf.keras.models.Model(inputs=[inlayer], outputs=[x_out])\n",
    "        return model\n",
    "   \n",
    "    gstore = GStore(BUCKET_NAME)\n",
    "    print('Evaluating Model...')\n",
    "    # Checking for edge cases\n",
    "    if processed_data_path == None or data_split_path == None:\n",
    "        raise ValueError('Evaluation failed: Component Configuration Invalid!')\n",
    "\n",
    "    if not gstore.is_dir(f'{processed_data_path}/processed'):\n",
    "        raise ValueError('Evaluation failed: Processed does not exist')\n",
    "\n",
    "    if not gstore.is_dir(f'{processed_data_path}/processed/numpy'):\n",
    "        raise ValueError('Evaluation failed: Processed data corrupted')\n",
    "\n",
    "    if not gstore.is_file(f'{processed_data_path}/processed/meta_data.json'):\n",
    "        raise ValueError('Evaluation failed: Processed data meta-data missing')    \n",
    "\n",
    "    if not gstore.is_file(f'{processed_data_path}/processed/processed_data.json'):\n",
    "        raise ValueError('Evaluation failed: Processed data missing')  \n",
    "    \n",
    "    if not gstore.is_file(f'models/latest.txt'):\n",
    "        raise ValueError('Evaluation failed: Latest model weights not found')\n",
    "\n",
    "    # Read processed data\n",
    "    meta_data = json.loads(gstore.download_into_memory(f'{processed_data_path}/processed/meta_data.json'))\n",
    "    ds = json.loads(gstore.download_into_memory(f'{processed_data_path}/processed/processed_data.json'))   \n",
    "\n",
    "    for header in meta_data:\n",
    "        if meta_data[header][0]:\n",
    "            ds[header] = np.load(io.BytesIO(gstore.download_into_memory(meta_data[header][2])), allow_pickle=True)\n",
    "\n",
    "    if not gstore.is_file(f'{data_split_path}/split_data_idx/train_idx.npy'):\n",
    "        raise ValueError('Training failed: Data splits could not be load')  \n",
    "        \n",
    "    if not gstore.is_file(f'{data_split_path}/split_data_idx/test_idx.npy'):\n",
    "        raise ValueError('Training failed: Data splits could not be load')  \n",
    "    \n",
    "    test_idx = np.load(io.BytesIO(gstore.download_into_memory(f'{data_split_path}/split_data_idx/test_idx.npy')), allow_pickle=True)\n",
    "    train_idx = np.load(io.BytesIO(gstore.download_into_memory(f'{data_split_path}/split_data_idx/train_idx.npy')), allow_pickle=True)  \n",
    "\n",
    "    # Check if we need to evaluate the latest model\n",
    "    if not model_path:\n",
    "        model_time = gstore.download_into_memory(f'models/latest.txt').decode()\n",
    "        model_path  = f'models/{model_time}'\n",
    "    \n",
    "    # Create temp local directory\n",
    "    local_model_path = os.path.join(*(['tmp'] + list(model_path.split('/'))+ ['no2_model.h5']))\n",
    "    if pathlib.Path(local_model_path).is_dir():\n",
    "       shutil.rmtree(local_model_path)\n",
    "    pathlib.Path(local_model_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if not gstore.is_file(f'{model_path}/no2_model.h5'):\n",
    "        raise ValueError('Evaluation failed: Model weights could not be load') \n",
    "    model = get_model_sequential((None, None, ds[\"sensor_responses\"][0].shape[1]), layers=layers)\n",
    "    # Download model weights to local directtory\n",
    "    gstore.download_file_local(f'{model_path}/no2_model.h5', local_model_path)\n",
    "    model.load_weights(local_model_path)\n",
    "\n",
    "\n",
    "    result_path = f'results/{os.path.basename(model_path)}'\n",
    "    if gstore.is_dir(result_path):\n",
    "        gstore.rmdir(result_path)\n",
    "    gstore.mkdir(result_path)\n",
    "\n",
    "    # Creating local output folders\n",
    "    plot_path = f'{result_path}/plots'\n",
    "    csv_path = f'{result_path}/csv'\n",
    "    results_path = f'{result_path}/results_path'\n",
    "    gstore.mkdir(plot_path)\n",
    "    gstore.mkdir(csv_path)\n",
    "    gstore.mkdir(results_path)\n",
    "\n",
    "    X = ds[\"sensor_responses\"]\n",
    "    X_train = X[train_idx]\n",
    "    X_test = X[test_idx]\n",
    "\n",
    "    y = ds[\"labels\"]\n",
    "    y_train = y[train_idx]\n",
    "    y_test = y[test_idx]\n",
    "\n",
    "    y_pred = model.predict(ds['sensor_responses'][test_idx])*div_by\n",
    "    y_test = y_test*div_by\n",
    "    ds['labels'] = ds['labels']*div_by\n",
    "    print(\"Prediction finished!\")\n",
    "\n",
    "    # Create temporary temp metrics folder\n",
    "    pathlib.Path(f'tmp/{results_path}').mkdir(parents=True, exist_ok=True)\n",
    "    with open(f'tmp/{results_path}/metrics.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for analyte in analytes:\n",
    "            device_id = []\n",
    "            analyte_GT_max = []\n",
    "            analyte_pred_max = []\n",
    "            R_squared = []\n",
    "            RMSE = []\n",
    "            delta_temp = []\n",
    "            delta_humid = []\n",
    "            filename = []\n",
    "            for i in range(len(y_pred)):\n",
    "                idx = test_idx[i]\n",
    "                if re.search('_'+analyte+'_', ds['run'][idx].lower()):\n",
    "                    last_idx = ds['last_index'][idx]\n",
    "                    plot_pred(y_pred[i], ds, test_idx[i], plot_path, csv_path, groundtruth=groundtruth)\n",
    "                    if groundtruth:\n",
    "                        R_squared.append(r2_score(y_test[i][2:last_idx], y_pred[i][2:last_idx]))\n",
    "                        RMSE.append(mean_squared_error(y_test[i][2:last_idx], y_pred[i][2:last_idx], squared=False))\n",
    "                        device_id.append(ds['moduleID'][idx])\n",
    "                        filename.append(ds['filename'][idx])\n",
    "                        analyte_GT_max.append(float(max(y_test[i][2:last_idx])))\n",
    "                        analyte_pred_max.append(float(max(y_pred[i][2:last_idx])))\n",
    "                        delta_temp.append(max(ds['temperature'][idx][2:last_idx]) - min(ds['temperature'][idx][2:last_idx]))\n",
    "                        delta_humid.append(max(ds['humidity'][idx][2:last_idx]) - min(ds['humidity'][idx][2:last_idx]))\n",
    "            if groundtruth:\n",
    "                scores_r2_mean = np.mean(R_squared)\n",
    "                scores_r2_std = np.std(R_squared)\n",
    "                row = f'{analyte} r2 scores mean = {scores_r2_mean}, std =  {scores_r2_std}'\n",
    "                writer.writerow([row])\n",
    "\n",
    "            if device_id:\n",
    "                results = pd.DataFrame(device_id)\n",
    "                results.columns = ['device_id']\n",
    "                results['filename'] = filename\n",
    "                results['R_squared'] = R_squared\n",
    "                results['RMSE'] = RMSE\n",
    "                results['analyte_GT_max'] = analyte_GT_max\n",
    "                results['analyte_pred_max'] = analyte_pred_max\n",
    "                results['delta_temp'] = delta_temp\n",
    "                results['delta_humid'] = delta_humid\n",
    "\n",
    "                results=results.sort_values(by=['R_squared'], ascending=False)\n",
    "                \n",
    "                results.to_csv(f'gs://{gstore.get_bucket_name()}/{results_path}/result_stats_{analyte}.csv', header=['device_id', 'filename', 'R_squared', 'RMSE', 'analyte_GT_max', 'analyte_pred_max', 'delta_temp', 'delta_humid'], index = False)\n",
    "\n",
    "        # # Save parameter file to folder\n",
    "        # with open(os.path.join(results_path, 'parameters.json'), 'w') as json_file:\n",
    "        #     json.dump(self.parameters, json_file)\n",
    "    gstore.upload_file_local(f'tmp/{results_path}/metrics.csv', f'{results_path}/metrics.csv')\n",
    "    # Delete temp local directory\n",
    "    if pathlib.Path('tmp').is_dir():\n",
    "       shutil.rmtree('tmp')\n",
    "    print('Model Evaluation Complete!')\n",
    "evaluate('data', 'data', groundtruth=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing End to End Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_end_to_end_pipeline():\n",
    "    read_postive_data('raw_data_sample/no2_positive', training=True)\n",
    "    read_negative_data('raw_data_sample/no2_negative', training=True)\n",
    "    merge_data('data', training=True)\n",
    "    process_data('data', training=True)\n",
    "    split_data('data', training=True)\n",
    "    model_training('data', 'data')\n",
    "    evaluate('data', 'data', groundtruth=True)\n",
    "\n",
    "test_end_to_end_pipeline()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0b4399f9436f06f6bf8413a286eb9d59c2f4d654f98f94dab8791cc0ced0958e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
